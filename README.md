# Neural Network From Scratch

In my last project, I made a simple handwritten digit recognizer using tensorflow. In this project, I will be recreating the same result _without_ tensorflow to get a better understanding of what goes on under the hood. (Also, jupyter notebook was pissing me off so I'm not using it anymore)

I had to use all my braincells for this one, but I also got understanding from these places:

  Gradient Descent (just do the negative to go in the opposite direction) [https://www.youtube.com/watch?v=_-02ze7tf08&list=PLSQl0a2vh4HC5feHa6Rc5c0wbRTx56nF7&index=20]
  
  The Calculus Stuff (just spam the chain rule) [https://www.youtube.com/watch?v=tIeHLnjs5U8&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&index=4]
  
  Derivative of Softmax (i didn't actually understand this one, and instead scrolled to the bottom and translated that into python) [https://mmuratarat.github.io/2019-01-27/derivation-of-softmax-function]

-AB

Completed 31/01/25
