class neural network
    initialize the layers (input size=784, hidden size=16, output size=10)
    initialize weights as random and biases as zero

    hot encode:
        if you input me a seven, i give you 0 for one, 0 for two...1 for SEVEN...

    forward:
        activation function hidden----relu = max(0, z)
        activation function softmax
            why use softmax instead of just (logits)/np.sum(logits)?
                it dosen't maintain relative confidence; if one value is super huge, it should weigh more than what it actually is if you take normal probability
                basically we want softmax because it exaggerates the values and makes our nn feel all nice and confident in its choices

    cost function
        sumof((softmax output - what i got from hot encode (0 or 1))^2)
        source: 3b1b 
    
    backprop
        gradient stuff, chain rule stuff... 

train it
    for epoch in range somenumberidk:
        get loss
        do backprop stuff & update weights/biases
        print: current epoch & accuracy

test on DEV SET
    get accuracy(maybe confusion matrix? we'll see)
    i don't really feel like doing this one and tbh for a nn of this complexity it's irrelavent so i prolly won't do it

SAVE nn using pickle (weights, biases, architecture,  etc)

-------diff file that calls the nn-------

pick a random img from test set 
ask nn what this img is

prediction = np.argmax(nn.forward(img))

plt.imshow(img,cmap='binary')
plt.title(f"Prediction: {prediction}")  
plt.show()
