class neural network
    initialize the layers (input size=784, hidden size=16, output size=10)
    initialize weights and biases 

    forward:
        activation function hidden----relu = max(0, z)
        activation function softmax
            why use softmax instead of just (logits)/np.sum(logits)?
                it dosen't maintain relative confidence; if one value is super huge, it should weigh more than what it actually is if you take normal probability
                basically we want softmax because it exaggerates the values and makes our nn feel all nice and confident in its choices

    cost function
        (some_output - what the output SHOULD be (0 or one))^2
        i'm pretty sure that's it unless i stupid 

    backprop
        output gradient
        hidden gradient w/ respect to output

        update weights and biases

    train it
        uhhh  do like 5 epochs

    test on DEV SET
        get accuracy(maybe confusion matrix? we'll see)

    predict when given an image
        put it thru forward prop
        use argmax to get the one which it's most confident with

    SAVE nn using pickle (weights, biases, architecture,  etc)

-------diff file that calls the nn-------

pick a random img from test set 
ask nn what this img is

plt.imshow(img,cmap='binary')
plt.title(f"Prediction: {prediction}")  
plt.show()